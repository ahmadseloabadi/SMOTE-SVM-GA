# -*- coding: utf-8 -*-
"""Salinan dari preprosessing text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1ggDoLlobFkm4UzHB-QBeXkrcayomSg
"""

!pip install Sastrawi
!pip install swifter
!pip install deap

import pandas as pd
import numpy as np
import re

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import swifter

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score

from sklearn import svm

"""# import dataset"""

#menghubungkan ke gdrive
from google.colab import drive
drive.mount('/content/drive')
#memasukan dataset
from google.colab import drive
ulasan = pd.read_csv('drive/MyDrive/SKRIPSI/kodingan/Ulasan My XL 1000 Data Labelled.csv')
ulasan.head(10)

# Commented out IPython magic to ensure Python compatibility.
# melihat jumlah kelas sentimen
Jumlah_sentimen = ulasan['Sentimen'].value_counts()
print("jumlah sentimen :")
print(Jumlah_sentimen)
# menampilkan dalam bentuk plot diagram
import matplotlib.pyplot as plt
# %matplotlib inline
labels = ['negative' , 'neutral', 'positive']
plt.pie(ulasan.groupby('Sentimen')['Sentimen'].count(), autopct=" %.1f%% " ,labels=labels)
plt.legend()
plt.show()

"""# cleansing"""

def cleaningulasan(ulasan):
  ulasan = re.sub(r'@[A-Za-a0-9]+',' ',ulasan)
  ulasan = re.sub(r'#[A-Za-z0-9]+',' ',ulasan)
  ulasan = re.sub(r"http\S+",' ',ulasan)
  ulasan = re.sub(r'[0-9]+',' ',ulasan)
  ulasan = re.sub(r"[-()\"#/@;:<>{}'+=~|.!?,_]", " ", ulasan)
  #menghapus karakter tunggal
  ulasan = re.sub(r"\b[a-zA-Z]\b", " ", ulasan)
  ulasan = ulasan.strip(' ')
  return ulasan
ulasan['Cleaning']= ulasan['Ulasan'].apply(cleaningulasan)


def clearEmoji(ulasan):
    return ulasan.encode('ascii', 'ignore').decode('ascii')
ulasan['HapusEmoji']= ulasan['Cleaning'].apply(clearEmoji)

def replaceTOM(ulasan):
    pola = re.compile(r'(.)\1{2,}', re.DOTALL)
    return pola.sub(r'\1', ulasan)
ulasan['cleansing']= ulasan['HapusEmoji'].apply(replaceTOM)

ulasan[['Ulasan','cleansing']]
ulasan[['Ulasan','cleansing']].to_csv('cleansingb.csv', index=False,float_format='%.2f')

"""# case folding"""

def casefoldingText(ulasan):
  ulasan = ulasan.lower()
  return ulasan
ulasan['CaseFolding']= ulasan['cleansing'].apply(casefoldingText)
ulasan[['cleansing','CaseFolding']]
ulasan[['cleansing','CaseFolding']].to_csv('casefolding.csv', index=False)

"""# tokenizing"""

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
def tokenizingText(ulasan):
  ulasan = word_tokenize(ulasan)
  return ulasan
ulasan['Tokenizing']= ulasan['CaseFolding'].apply(tokenizingText)
ulasan[['CaseFolding','Tokenizing']]
ulasan[['CaseFolding','Tokenizing']].to_csv('tokenizing.csv', index=False)

"""# stemming"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import swifter

factory = StemmerFactory()
stemmer = factory.create_stemmer()

#mengubah sebuah kalimat atau teks menjadi kata dasar
def stemmed_wrapper(term):
    return stemmer.stem(term)

term_dict = {}
#memasukan kata ke variable term_dict
for document in ulasan['Tokenizing']:
    for term in document:
        if term not in term_dict:
            term_dict[term] = ' '

print(len(term_dict))
print("------------------------")

for term in term_dict:
    term_dict[term] = stemmed_wrapper(term)
print(term,":" ,term_dict[term])

print(term_dict)
print("------------------------")
# Fungsi untuk mengubah kalimat menjadi kata dasar dan menangani negasi pada kalimat
def stemmingText(document):
    return [term_dict[term] for term in document]

ulasan['Stemming_list'] = ulasan['Tokenizing'].apply(stemmingText)

#merubah list ke str
ulasan['Stemming'] = ulasan['Stemming_list'].apply(' '.join)

#menampilkan data hasil stemming
ulasan[['Tokenizing','Stemming']]
ulasan[['Tokenizing','Stemming']].to_csv('stemming.csv', index=False)

"""# negasi handling"""

def handle_negation(kalimat_baru):
    negation_words = ["tidak", "bukan", "tak", "tiada", "jangan", "gak"]
    new_words = []
    prev_word_is_negation = False
    for word in kalimat_baru:
        if word in negation_words:
            new_words.append("tidak_")
            prev_word_is_negation = True
        elif prev_word_is_negation:
            new_words[-1] += word
            prev_word_is_negation = False
        else:
            new_words.append(word)
    return new_words
ulasan['negasi'] = ulasan['Stemming_list'].apply(handle_negation)
#menampilkan data hasil stemming
ulasan[['Stemming','negasi']]
ulasan[['Stemming','negasi']].to_csv('negasi.csv', index=False)

"""# word normalization"""

def convertToSlangword(ulasan):
    kamusSlang = eval(open("drive/MyDrive/SKRIPSI/kodingan/slangwords.txt").read())
    pattern = re.compile(r'\b( ' + '|'.join (kamusSlang.keys())+r')\b')
    content = []
    for kata in ulasan:
        filter_slang = pattern.sub(lambda x: kamusSlang[x.group()], kata.lower())
        if filter_slang.startswith('tidak_'):
          kata_depan = 'tidak_'
          kata_belakang = kata[6:]
          kata_belakang_slang = pattern.sub(lambda x: kamusSlang[x.group()], kata_belakang.lower())
          kata_hasil = kata_depan + kata_belakang_slang
          content.append(kata_hasil)
        else:
          content.append(filter_slang)
    ulasan = content
    return ulasan

ulasan['Formalisasi'] = ulasan['negasi'].apply(convertToSlangword)
ulasan[['negasi','Formalisasi']]
ulasan[['negasi','Formalisasi']].to_csv('word normalization.csv', index=False)

"""# stopword removal"""

import pandas as pd
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Mengunduh daftar stopword dari NLTK
daftar_stopword = stopwords.words('indonesian')

# Menambahkan stopword tambahan
daftar_stopword.extend(["yg", "dg", "rt", "dgn", "ny", "d",'gb','ahk','g'])

# Membaca file teks stopword menggunakan pandas
txt_stopword = pd.read_csv("drive/MyDrive/SKRIPSI/kodingan/stopwords.txt", names=["stopwords"], header=None)

# Menggabungkan daftar stopword dari NLTK dengan daftar stopword dari file teks
daftar_stopword.extend(txt_stopword['stopwords'].tolist())

# Mengubah daftar stopword menjadi set untuk pencarian yang lebih efisien
daftar_stopword = set(daftar_stopword)

def stopwordText(words):
    cleaned_words = []
    for word in words:
        # Memisahkan kata dengan tambahan "tidak_"
        if word.startswith("tidak_"):
            cleaned_words.append(word[:5])
            cleaned_words.append(word[6:])
        elif word not in daftar_stopword:
            cleaned_words.append(word)
    return cleaned_words

ulasan['Stopword Removal'] = ulasan['Formalisasi'].apply(stopwordText)
ulasan[['Formalisasi','Stopword Removal']].to_csv('stopword.csv', index=False)

ulasan.to_csv('dataset_bersih.csv',index=False)

"""# pembobotan tf-idf"""

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer
from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

X = ulasan['Stemming']
Y = ulasan['Sentimen']

x_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.25)

vectorizer = TfidfVectorizer()
x_train = vectorizer.fit_transform(x_train)
x_test = vectorizer.transform(x_test)
Encoder = LabelEncoder()
y_train = Encoder.fit_transform(y_train)
y_test = Encoder.fit_transform(y_test)


# Create CountVectorizer instance
count_vectorizer = CountVectorizer()
X_count = count_vectorizer.fit_transform(X)

# Create TfidfTransformer instance
tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_count)

# Create TfidfVectorizer instance
tfidf_vectorizer = TfidfVectorizer()
X_tfidf_vectorized = tfidf_vectorizer.fit_transform(X)

# Get the feature names from CountVectorizer or TfidfVectorizer
feature_names = count_vectorizer.get_feature_names_out()  # or tfidf_vectorizer.get_feature_names()

# Create a dictionary to store the results
results = {"Ulasan": [], "Term": [], "TF": [], "IDF": [], "TF-IDF": []}

# Loop over the documents
for i in range(len(X)):
    # Add the document to the results dictionary
    results["Ulasan"].extend([f" ulasan{i+1}"] * len(feature_names))
    # Add the feature names to the results dictionary
    results["Term"].extend(feature_names)
    # Calculate the TF, IDF, and TF-IDF for each feature in the document
    for j, feature in enumerate(feature_names):
        tf = X_count[i, j]
        idf = tfidf_transformer.idf_[j]  # or X_tfidf_vectorized.idf_[j]
        tf_idf_score = X_tfidf[i, j]  # or X_tfidf_vectorized[i, j]
        # Add the results to the dictionary
        results["TF"].append(tf)
        results["IDF"].append(idf)
        results["TF-IDF"].append(tf_idf_score)
# Convert the results dictionary to a Pandas dataframe
df = pd.DataFrame(results)


# Save the results to a CSV file
df.to_csv("tf_idf_results.csv", index=False)

#filter nilai term
newdf = df[(df.TF != 0 )]
newdf
# Save the results to a CSV file
newdf.to_csv("hasil TF IDF.csv", index=False)

x_train.shape

from sklearn.model_selection import KFold
from sklearn import svm
from sklearn.svm import SVC

x =vectorizer.fit_transform(ulasan['Stemming'])
y =Encoder.fit_transform(ulasan['Sentimen'])

kfold=5
kf = KFold(n_splits=kfold)
sum_of_error = 0
hasilakurasigasvm = []
hasilakurasisvm = []

for train_index,test_index in kf.split(x,y):
    x_train,x_test = x[train_index],x[test_index]
    y_train,y_test = y[train_index],y[test_index]

    normsvm = svm.SVC(kernel='linear')
    gasvm = svm.SVC(kernel='rbf')

    gasvm.fit(x_train,np.ravel(y_train))
    akurasigasvm = gasvm.score(x_test,y_test)
    hasilakurasigasvm.append(akurasigasvm)


    normsvm.fit(x_train,np.ravel(y_train))
    akurasisvm = normsvm.score(x_test,y_test)
    hasilakurasisvm.append(akurasisvm)


# Menampilkan plot akurasi
plt.plot(range(1, 6), np.array(hasilakurasigasvm)*100, marker='o', label='GA-SVM')
for x,y in zip(range(1, 6), np.array(hasilakurasigasvm)*100):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
# Menampilkan plot akurasi
plt.plot(range(1, 6), np.array(hasilakurasisvm)*100, marker='o', label='SVM')
for x,y in zip(range(1, 6), np.array(hasilakurasisvm)*100):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.xlabel('Fold')
plt.ylabel('Akurasi')
plt.legend(loc='best')
plt.title('Plot Akurasi K-Fold Cross Validation (k = 5)')
plt.grid(True)
plt.show()

# plt.plot(kfold,  np.array(rec_svm)*100, '-o', label='SVM')
# for x,y in zip(kfold, np.array(rec_svm)*100):
#     plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)

# plt.plot(kfold,  np.array(rec_gasvm)*100, '-o', label='GA-SVM')
# for x,y in zip(kfold, np.array(rec_gasvm)*100):
#     plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)

# plt.xlabel('K-fold')
# plt.ylabel('recall')


# plt.show()

"""# klasifikasi svm"""

from sklearn.metrics import confusion_matrix, accuracy_score

# Making the SVM Classifer
Classifier = svm.SVC()

# Training the model on the training data and labels
Classifier.fit(x_train, y_train)

# Using the model to predict the labels of the test data
y_pred = Classifier.predict(x_test)

# import pickle
# pickle.dump(Classifier, open("svm_new", "wb"))
# Evaluating the accuracy of the model using the sklearn functions
accuracy = accuracy_score(y_test,y_pred)*100

clf = svm.SVC(class_weight='balanced')
clf.fit(x_train, y_train)

# Using the model to predict the labels of the test data
pred = clf.predict(x_test)

# import pickle
# pickle.dump(clf, open("svm_new", "wb"))
# Evaluating the accuracy of the model using the sklearn functions
acc = accuracy_score(y_test,pred)*100


# Printing the results
print("Accuracy for SVM is:",accuracy)
print("Accuracy for SVM balanced is:",acc)

# import seaborn as sns
# import matplotlib.pyplot as plt
# f, ax = plt.subplots(figsize=(8,5))
# sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=".0f", ax=ax)
# plt.xlabel("y_head")
# plt.ylabel("y_true")
# plt.show()
# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
# cm_df = pd.DataFrame(conf_matrix,
#                      index = ['Positif','Netral','VIRGINICA'],
#                      columns = ['SETOSA','VERSICOLR','VIRGINICA'])

"""# smote"""

x_res =vectorizer.fit_transform(ulasan['Stemming'])
y_res =ulasan['Sentimen']

from imblearn.over_sampling import SMOTE
from collections import Counter
import csv

print(f'Dataset sebelum SMOTE : {Counter(Y)}')

#penerapan smote
smote = SMOTE(sampling_strategy='auto')
X_smote, Y_smote = smote.fit_resample(x_res, y_res)

# Count original and synthetic data
counts = pd.DataFrame({'target': Y_smote}).value_counts().reset_index(name='counts')

print(sorted(Counter(Y_smote).items()))

df = pd.DataFrame(X_smote)
df.rename(columns={0:'term'}, inplace=True)
df['sentimen'] = Y_smote
# mengembalikan kalimat asli dari tfidf
feature_names = vectorizer.get_feature_names_out()

kalimat_asli = []
for index, row in df.iterrows():
    vektor_ulasan = X_smote[index]
    kata_kunci = [feature_names[i] for i in vektor_ulasan.indices]
    kalimat_asli.append(' '.join(kata_kunci))

# tambahkan kolom baru dengan kalimat asli ke dalam data frame
df['kalimat_asli'] = kalimat_asli
df.to_csv('data_smote.csv', index=False)
#mengambil data sintetik
df_sintetik = df.iloc[1000:]
#menyimpan dalam bentuk csv
df_sintetik.to_csv('data_sintetik.csv', index=False)
df

X_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, test_size=0.20)

"""# svm+smote"""

# !pip install scipy
import scipy
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn import svm
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report

# Making the SVM Classifer
classifier = svm.SVC(kernel='poly',degree=1)

# Training the model on the training data and labels
classifier.fit(X_train, Y_train)

# Using the model to predict the labels of the test data
Y_predsvm = classifier.predict(X_test)

# Evaluating the accuracy of the model using the sklearn functions
report = classification_report(Y_test,Y_predsvm)
report
# Printing the results
print("Accuracy for SVM is:",report)

print(classifier.get_params())

data=scipy.sparse.csr_matrix.toarray(X_train)
pca = PCA(n_components = 3).fit(data)
X_pca = pca.transform(data)

import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from deap import base, creator, tools


# Membuat kromosom dan fitness function
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

# Membuat toolbox
toolbox = base.Toolbox()
toolbox.register("attr_float", np.random.uniform, 0.1, 10)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=3)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# Membuat fitness function
def evalSVM(individual):
    C, gamma, coef0 = individual
    svm = SVC(C=C, gamma=gamma, coef0=coef0)
    svm.fit(X_train, Y_train)
    y_pred = svm.predict(X_test)
    acc = accuracy_score(Y_test, y_pred)
    return acc,

toolbox.register("evaluate", evalSVM)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)
toolbox.register("select", tools.selTournament, tournsize=3)

# Membuat populasi awal
pop = toolbox.population(n=20)

# Menentukan jumlah generasi
num_generations = 50
# Menentukan jumlah crosover PROBABILITY
CXPB = 0.5
# Menentukan jumlah mutasi PROBABILITY
MUTPB = 0.2
# Melakukan evolusi

for i in range(num_generations):
    offspring = toolbox.select(pop, len(pop))
    offspring = list(map(toolbox.clone, offspring))

    for child1, child2 in zip(offspring[::2], offspring[1::2]):
        if np.random.random() < CXPB:
            toolbox.mate(child1, child2)
            del child1.fitness.values
            del child2.fitness.values

    for mutant in offspring:
        if np.random.random() < MUTPB:
            toolbox.mutate(mutant)
            del mutant.fitness.values

    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit

    pop[:] = offspring
    fits = [ind.fitness.values[0] for ind in pop]
    length = len(pop)
    mean = sum(fits) / length
    sum2 = sum(x*x for x in fits)
    std = abs(sum2 / length - mean**2)**0.5
    print("  Min %s" % min(fits))
    print("  Max %s" % max(fits))
    print("  Avg %s" % mean)
    print("  Std %s" % std)
    print("  C value %s" % pop[fits.index(max(fits))][0])
    print("  Gamma value %s" % pop[fits.index(max(fits))][1])
    best_fit = pop[np.argmax([ind.fitness.values[0] for ind in pop])]

#Menampilkan hasil PARAMETER
print("Hasil perulangan dari algoritma genetika:")
print("C:", best_fit[0])
print("gamma:", best_fit[1])
print("coef0:", best_fit[2])
#Use the best individual to classify the test dataset
clf = svm.SVC(C=best_fit[0], gamma=best_fit[1],coef0=best_fit[2])
clf.fit(X_train, Y_train)
test_accuracy = clf.score(X_test, Y_test)
#MENAMPILKAN HASIL AKURASI genetic algorithm
print("Best individual:", best_fit)
print("Test accuracy:", test_accuracy)

from sklearn.model_selection import KFold
from sklearn import svm
from sklearn.svm import SVC
from sklearn.metrics import recall_score,precision_score

kfold=5
kf = KFold(n_splits=kfold)

hasilakurasigasvm = []
hasilakurasisvm = []

precision_scoresgasvm = []
precision_scoressvm = []

recall_scoresgasvm = []
recall_scoressvm = []

for train_index,test_index in kf.split(X_smote,Y_smote):
    x_train,x_test = X_smote[train_index],X_smote[test_index]
    y_train,y_test = Y_smote[train_index],Y_smote[test_index]

    normsvm = svm.SVC()
    gasvm = svm.SVC(kernel='rbf',C=4.687022672477756,gamma=1.241440234431267)

    gasvm.fit(x_train,np.ravel(y_train))
    akurasigasvm = gasvm.score(x_test,y_test)
    hasilakurasigasvm.append(akurasigasvm)
    Y_pred = gasvm.predict(x_test)
    precision_scoresgasvm.append(precision_score(y_test, Y_pred,average='macro'))
    recall_scoresgasvm.append(recall_score(y_test, Y_pred,average='macro'))

    normsvm.fit(x_train,np.ravel(y_train))
    akurasisvm = normsvm.score(x_test,y_test)
    hasilakurasisvm.append(akurasisvm)
    y_pred = normsvm.predict(x_test)
    precision_scoressvm.append(precision_score(y_test, y_pred,average='macro'))
    recall_scoressvm.append(recall_score(y_test, y_pred,average='macro'))


# Menampilkan plot akurasi
plt.plot(range(1, 6), np.array(hasilakurasigasvm)*100, marker='o', label='GA-SVM')
for x,y in zip(range(1, 6), np.array(hasilakurasigasvm)*100):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
# Menampilkan plot akurasi
plt.plot(range(1, 6), np.array(hasilakurasisvm)*100, marker='o', label='SVM')
for x,y in zip(range(1, 6), np.array(hasilakurasisvm)*100):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.xlabel('Fold')
plt.ylabel('Akurasi')
plt.legend(loc='best')
plt.title('Plot Akurasi K-Fold Cross Validation (k = 5)')
plt.grid(True)
plt.savefig('akurasi_plot.png')
plt.show()


# Plotting precision
plt.plot(range(1, 6), np.array(precision_scoresgasvm)*100, marker='o', label='GA-SVM')
for x,y in zip(range(1, 6), np.array(precision_scoresgasvm)*100):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.plot(range(1, 6), np.array(precision_scoressvm)*100, marker='o', label='SVM')
for x,y in zip(range(1, 6), np.array(precision_scoressvm)*100):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.xlabel('Fold')
plt.ylabel('precision')
plt.legend(loc='best')
plt.title('Plot precision K-Fold Cross Validation (k = 5)')
plt.grid(True)
plt.savefig('precision_plot.png')
plt.show()

# Plotting recall
plt.plot(range(1, 6), np.array(recall_scoresgasvm)*100, marker='o', label='GA-SVM')
for x,y in zip(range(1, 6), np.array(recall_scoresgasvm)*100):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.plot(range(1, 6), np.array(recall_scoressvm)*100, marker='o', label='SVM')
for x,y in zip(range(1, 6), np.array(recall_scoressvm)*100):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.xlabel('Fold')
plt.ylabel('recall')
plt.legend(loc='best')
plt.title('Plot recall K-Fold Cross Validation (k = 5)')
plt.grid(True)
plt.savefig('recall_plot.png')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, KFold, cross_val_predict
from sklearn.metrics import recall_score,precision_score

k = 5

acc_svm=cross_val_score(clf, x_train, y_train, cv=k)
acc_psosvm=cross_val_score(classifier, x_train, y_train, cv=k)

plt.plot(range(1, k+1),acc_svm, '-o', label='SVM')
for x,y in zip(range(1, k+1),acc_svm):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)

plt.plot(range(1, k+1), acc_psosvm, '-o', label='PSO-SVM')
for x,y in zip(range(1, k+1),acc_psosvm):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.xlabel('K-fold')
plt.ylabel('Accuracy')
plt.legend(loc='best')
plt.savefig('kfold_cross_validation.png')
plt.show()

prec_svm = cross_val_score(clf, x_train, y_train, cv=k,scoring='precision_macro')
prec_psosvm = cross_val_score(classifier, x_train, y_train, cv=k,scoring='precision_macro')

plt.plot(range(1, k+1),prec_svm, '-o', label='SVM')
for x,y in zip(range(1, k+1),prec_svm):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)

plt.plot(range(1, k+1),prec_psosvm, '-o', label='PSO-SVM')
for x,y in zip(range(1, k+1),prec_psosvm):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.xlabel('K-fold')
plt.ylabel('Precision')
plt.legend(loc='best')
plt.show()


rec_svm = cross_val_score(clf, x_train, y_train, cv=k,scoring='recall_macro')
rec_psosvm = cross_val_score(classifier, x_train, y_train, cv=k,scoring='recall_macro')

plt.plot(range(1, k+1),rec_svm, '-o', label='SVM')
for x,y in zip(range(1, k+1),rec_svm):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)

plt.plot(range(1, k+1),rec_psosvm, '-o', label='PSO-SVM')
for x,y in zip(range(1, k+1),rec_psosvm):
    plt.text(x, y, '{:.2f}'.format(y), ha='center', va='bottom', fontsize=10)
plt.xlabel('K-fold')
plt.ylabel('Recall')
plt.legend(loc='best')
plt.show()

from sklearn.metrics import accuracy_score, recall_score, f1_score,precision_score, classification_report
from sklearn.svm import SVC


gamma_values = [0.1,0.99,1,5,10,100]
c=1

# inisialisasi list untuk menyimpan hasil
results = []

# ulangi model SVM dengan setiap nilai C dan gamma pada rentang yang ditentukan
for gamma in gamma_values:
    # inisialisasi model SVM dengan kernel RBF
    clasi = svm.SVC(kernel='rbf', C=c, gamma=gamma)

    # latih model dengan data training
    clasi.fit(X_train, Y_train)

    # prediksi label pada data testing
    y_pred = clasi.predict(X_test)

    # hitung metrik evaluasi model
    accuracy = accuracy_score(Y_test, y_pred)*100
    precision = precision_score(Y_test, y_pred, average='weighted')*100
    recall = recall_score(Y_test, y_pred, average='weighted')*100
    # tambahkan hasil ke dalam list
    results.append({'c': c, 'gamma': gamma, 'accuracy': accuracy,'precision':"{:.2f}".format(precision), 'recall': recall })

# simpan hasil ke dalam file CSV
df = pd.DataFrame(results)
df.to_csv('hasil_svm_Gamma.csv', index=False)

from sklearn.metrics import accuracy_score, recall_score, f1_score,precision_score, classification_report
from sklearn.svm import SVC

c_values = [0.1,0.5,1,5,10,25,50,100]

gamma=0.1

# inisialisasi list untuk menyimpan hasil
results = []

# ulangi model SVM dengan setiap nilai C dan gamma pada rentang yang ditentukan
for c in c_values:
    # inisialisasi model SVM dengan kernel RBF
    clasi = svm.SVC(kernel='rbf', C=c, gamma=gamma)

    # latih model dengan data training
    clasi.fit(X_train, Y_train)

    # prediksi label pada data testing
    y_pred = clasi.predict(X_test)

    # hitung metrik evaluasi model
    accuracy = accuracy_score(Y_test, y_pred)*100
    precision = precision_score(Y_test, y_pred, average='weighted')*100
    recall = recall_score(Y_test, y_pred, average='weighted')*100
    # tambahkan hasil ke dalam list
    results.append({'c': c, 'gamma': gamma, 'accuracy': accuracy,'precision':"{:.2f}".format(precision), 'recall': recall})

# simpan hasil ke dalam file CSV
df = pd.DataFrame(results)
df.to_csv('hasil_svm_c.csv', index=False)

from sklearn.metrics import accuracy_score, recall_score, f1_score,precision_score, classification_report
from sklearn.svm import SVC

kernel_values = ['linear','rbf','poly','sigmoid']

# inisialisasi list untuk menyimpan hasil
results = []

# ulangi model SVM dengan setiap nilai C dan gamma pada rentang yang ditentukan
for k in kernel_values:
    # inisialisasi model SVM dengan kernel RBF
    clasi = svm.SVC(kernel=k)

    # latih model dengan data training
    clasi.fit(X_train, Y_train)

    # prediksi label pada data testing
    y_pred = clasi.predict(X_test)

    # hitung metrik evaluasi model
    accuracy = accuracy_score(Y_test, y_pred)*100
    precision = precision_score(Y_test, y_pred, average='weighted')*100
    recall = recall_score(Y_test, y_pred, average='weighted')*100
    # tambahkan hasil ke dalam list
    results.append({'kernel':k ,'accuracy': accuracy,'precision':"{:.2f}".format(precision), 'recall': recall})

# simpan hasil ke dalam file CSV
df = pd.DataFrame(results)
df.to_csv('hasil_svm_kernel.csv', index=False)

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
from sklearn.svm import SVC

# Mempersiapkan data untuk SVM
iris = datasets.load_iris()
X = iris.data[:, :3]  # Hanya menggunakan tiga fitur pertama untuk visualisasi
y = iris.target

# Melatih model SVM dengan kernel RBF
svm = SVC(kernel='rbf')
svm.fit(X, y)

# Mendapatkan support vectors
support_vectors = svm.support_vectors_

# Membuat plot 3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Menggambarkan data dalam tiga dimensi
colors = ['r', 'g', 'b']
markers = ['o', '^', 's']
for i, color, marker in zip(range(3), colors, markers):
    ax.scatter(X[y == i, 0], X[y == i, 1], X[y == i, 2], c=color, marker=marker)

# Menghitung grid untuk menggambar permukaan pemisah
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
z_min, z_max = X[:, 2].min() - 1, X[:, 2].max() + 1
xx, yy, zz = np.meshgrid(np.linspace(x_min, x_max, 10),
                         np.linspace(y_min, y_max, 10),
                         np.linspace(z_min, z_max, 10))
grid = np.c_[xx.ravel(), yy.ravel(), zz.ravel()]

# Mengklasifikasikan setiap grid point
Z = svm.predict(grid)
Z = Z.reshape(xx.shape)

# Menggambar permukaan pemisah dengan warna berbeda untuk setiap kelas
ax.scatter(grid[:, 0], grid[:, 1], grid[:, 2], c=Z, cmap='viridis', alpha=0.5)

# Menggambar support vectors
ax.scatter(support_vectors[:, 0], support_vectors[:, 1], support_vectors[:, 2],
           facecolors='none', edgecolors='k', s=100)

# Menambahkan label sumbu
ax.set_xlabel('Fitur 1')
ax.set_ylabel('Fitur 2')
ax.set_zlabel('Fitur 3')

# Menampilkan plot
plt.show()